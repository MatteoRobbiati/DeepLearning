{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30721a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numba as nb\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(font_scale=1.7, style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29551a42",
   "metadata": {},
   "source": [
    "# Esercitazione 3\n",
    "\n",
    "## 0 - Appunti e introduzione\n",
    "\n",
    "Nella scorsa lezione abbiamo introdotto alcune strategie per aumentare le performance dei codici scritti in `Python`, tra cui in particolare utilizziamo numba. \n",
    "\n",
    "Estendere il `back-end` utilizzando altre strategie è importante. Se necessito una fz molto performante su `tf` posso, ad esempio, scriverla in `C++` e poi fornirla a `tf`.\n",
    "\n",
    "#### Che framework scegliere?\n",
    "\n",
    "Ci sono un sacco di opzioni. Diciamo che `tf` è il più utilizzato tra tutti. Seguono `keras` e `PyTorch`. Per scegliere conviene analizzare _learning curve, developement pace, community size, papers associate al framework (tf ad esempio consente agli utenti di interagire ed aprire degli issues su GitHub), stabilità nel tempo e performance_.\n",
    "\n",
    "---\n",
    "\n",
    "### Su TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdee8c05",
   "metadata": {},
   "source": [
    "#### Variabili e tensori\n",
    "\n",
    "Posso decidere dove e come allocare la memoria. se scrivo `with tf.device('CPU:0')` sto scegliendo la CPU. Alternativamente `same... ('GPU:0')`.\n",
    "\n",
    "- `tf.Variable` è una variabile il cui valore è modificabile in futuro utilizzando `tf.assign()`;\n",
    "- `tf.constant` è una variabile il cui valore viene mantenuto costante.\n",
    "\n",
    "In `C++` non potrei fare questa operazione in meno di 50 righe.\n",
    "\n",
    "#### Gradienti\n",
    "\n",
    "Utilizzando `GradientTape` tf interpreta il gradiente della formula funzionale. \n",
    "\n",
    "HP di avere un modello lineare, in cui definisco una matrice di pesi e una di bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4474fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random.normal((3,2)), name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "x = [[1.,2.,3.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbbacbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y = tf.nn.sigmoid(x @ w + b)\n",
    "    loss = tf.reduce_mean(tf.math.square(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5468d6b7",
   "metadata": {},
   "source": [
    "Abbiamo costruito una loss che è una catena di operazioni. Abbiamo in qualche modo costruito un algoritmo di stocastic gradient descent in completa autonomia. Nelle slide fa un esempio di training su quattro variabili in cui si vede come il `tape` vada a tener conto solo di quelle che rimangono a tutti gli effetti delle `Variable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d2cf0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[dl_dw, dl_db] = tape.gradient(loss, [w, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36f53026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.00206387 0.00033378]\n",
      " [0.00412774 0.00066756]\n",
      " [0.00619162 0.00100134]], shape=(3, 2), dtype=float32) tf.Tensor([0.00206387 0.00033378], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(dl_dw, dl_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b75025",
   "metadata": {},
   "source": [
    "#### tf Module\n",
    "Possiamo costruire una classe (di seguito alcune note utili se, come me, sei nabb*):\n",
    "\n",
    "- `__init__` è un costruttore. Metodo invocato automaticamente nel momento in cui si va definire un oggetto che corrisponde alla classe in esame. Contiene i vari data membri, che verranno richiamati tramite il prefisso `self.`;\n",
    "- `super()` invoca la classe madre quando stiamo programmando utilizzando ereditareità nelle classi. Può essere usato per evitare di ricordarsi il nome della super-classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ba811d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModule(tf.Module):\n",
    "    \n",
    "    #costruttore \n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.a_variable = tf.Variable(5.0, name='train_me')\n",
    "        self.non_trainable_variable = tf.Variable(5.0, trainable=False)\n",
    "    \n",
    "    #azione in chiamata della classe\n",
    "    def __call__(self, x):\n",
    "        return self.a_variable * x + self.non_trainable_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7080e1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=30.0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_module = SimpleModule(name=\"simple\")\n",
    "simple_module(tf.constant(5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f4c8711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se voglio costruire un layer dense\n",
    "\n",
    "class Dense(tf.Module):\n",
    "    \n",
    "    #costruttore\n",
    "    def __init__(self, in_features, out_features, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.w = tf.Variable(tf.random.normal([in_features, out_features]), name='w')\n",
    "        self.b = tf.Variable(tf.zeros([out_feature]), name='b')\n",
    "    \n",
    "    # azione in chiamata della classe\n",
    "    def __call__(self, x):\n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "        return tf.nn.relu(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44d5342",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Su Keras\n",
    "\n",
    "Keras rientra nel mondo del high-level. Oggi è un modulo di `tf` che lavora come API. Un esempio di costruzione tramite Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff56b5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (1, 2)                    4         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (1, 3)                    9         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (1, 1)                    4         \n",
      "=================================================================\n",
      "Total params: 17\n",
      "Trainable params: 17\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(2, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(3, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "# ora la rete è inizializzata in modo casuale dal framework\n",
    "# se la alleniamo questa si adegua ai pattern del mio campione\n",
    "# però già così una risposta me la da, casuale ma me la da\n",
    "\n",
    "y = model(tf.ones((1,1)))\n",
    "model.summary()\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c9bfe",
   "metadata": {},
   "source": [
    "A questo punto abbiamo due metodi necessari all'utilizzo della rete:\n",
    "\n",
    "1. `model.compile()` compila la rete;\n",
    "2. `model.fit()` la allena.\n",
    "\n",
    "Questa tecnica sequenziale potrebbe non essere sempre utile. Ad esempio quando:\n",
    "\n",
    "- abbiamo output e input multipli;\n",
    "- ogni layer ha multiple inputs and outputs;\n",
    "- vogliamo condivisione tra i layers;\n",
    "- vogliamo una topologia non lineare.\n",
    "\n",
    "Ecco che entra in gioco la __API funzionale__. Usando tf dichiariamo un input e indichiamo la shape. A questo punto definiamo i layers separatamente e passiamo manualmente come input degli strati successivi gli output dei precedenti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e7048c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(5,))\n",
    "x1 = tf.keras.layers.Dense(64, activation='relu')(inputs)\n",
    "x2 = tf.keras.layers.Dense(64, activation='relu')(x1)\n",
    "outputs = tf.keras.layers.Dense(1)(x2)\n",
    "\n",
    "# e adesso trasformo la mia struttura in un modulo standard, così posso usare i metodi\n",
    "# compile e fit dalla stessa API vista prima\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"MyModel\")\n",
    "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.SGD())\n",
    "\n",
    "# e se carico anche i dataset a questo punto posso eseguire il training\n",
    "#history = model.fit(x_train, y_train, batch_size=32, epochs=100)\n",
    "\n",
    "# e poi valutare il modello con evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de76c198",
   "metadata": {},
   "source": [
    "## 1 - MLP\n",
    "\n",
    "Per questo primo esercizio costruisco una classe che chiamo simpleMLP.\n",
    "\n",
    "In --> h1 --> h2 --> Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f4463b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 10, dtype=np.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ccf9f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "class simpleMLP:\n",
    "    \n",
    "    # costruisco tutti i pesi nella rete\n",
    "    def __init__(self, n_input, n_output, n_hidden_1, n_hidden_2):\n",
    "        # In --> h1    \n",
    "        self.w1 = tf.Variable(tf.random.normal([n_input, n_hidden_1]), name='w1')\n",
    "        self.b1 = tf.Variable(tf.random.normal([n_hidden_1]), name='b1')\n",
    "        \n",
    "        # h1 --> h2\n",
    "        self.w2 = tf.Variable(tf.random.normal([n_hidden_1, n_hidden_2]), name='w2')\n",
    "        self.b2 = tf.Variable(tf.random.normal([n_hidden_2]), name='b2')\n",
    "        \n",
    "        # h2 --> Out\n",
    "        self.w3 = tf.Variable(tf.random.normal([n_hidden_2, n_output]), name='w3')\n",
    "        self.b3 = tf.Variable(tf.random.normal([n_output]), name='b3')\n",
    "        \n",
    "    # chiamata della rete sul dato x    \n",
    "    def __call__(self, x):\n",
    "        a1 = tf.nn.sigmoid(tf.matmul(x, self.w1) + self.b1)\n",
    "        a2 = tf.nn.sigmoid(tf.matmul(a1, self.w2) + self.b2)\n",
    "        a3 = tf.matmul(a2, self.w3) + self.b3\n",
    "        return a3\n",
    "    \n",
    "    def get_weights(self):\n",
    "        w = []\n",
    "        b = []\n",
    "        for i in range(3):\n",
    "            w.append(self.w1)\n",
    "            b.append\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a1b451f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input, output, hidden1, hidden1\n",
    "first_mlp = simpleMLP(1, 1, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "da4382fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
       "array([[1.3699194 ],\n",
       "       [1.2839926 ],\n",
       "       [1.1930858 ],\n",
       "       [1.101919  ],\n",
       "       [1.0151278 ],\n",
       "       [0.9362234 ],\n",
       "       [0.86713374],\n",
       "       [0.80833006],\n",
       "       [0.7592629 ],\n",
       "       [0.7188215 ]], dtype=float32)>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f8978d",
   "metadata": {},
   "source": [
    "## 2 - Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "31dbd3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 5)                 10        \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 2)                 12        \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# per far sì che i pesi siano inizializzati nello stesso modo\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "second_mlp = tf.keras.Sequential()\n",
    "second_mlp.add(tf.keras.layers.Dense(5, input_shape=(1,), activation=\"sigmoid\"))\n",
    "second_mlp.add(tf.keras.layers.Dense(2, activation=\"sigmoid\"))\n",
    "second_mlp.add(tf.keras.layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "second_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "84cbf8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
       "array([[-0.08518803],\n",
       "       [-0.07886216],\n",
       "       [-0.07243189],\n",
       "       [-0.06592345],\n",
       "       [-0.05936563],\n",
       "       [-0.05278838],\n",
       "       [-0.04622245],\n",
       "       [-0.03969827],\n",
       "       [-0.03324482],\n",
       "       [-0.02688947]], dtype=float32)>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22e3271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
